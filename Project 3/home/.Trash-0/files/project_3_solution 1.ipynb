{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Smart Beta Portfolio and Portfolio Optimization\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to use the [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/) packages. Don't import any other packages, otherwise the grader willn't be able to run your code.\n",
    "\n",
    "The other packages that we're importing is `helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper\n",
    "import project_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Data\n",
    "The data source we'll be using is the [Wiki End of Day data](https://www.quandl.com/databases/WIKIP) hosted at [Quandl](https://www.quandl.com). This contains data for many stocks, but we'll just be looking at the S&P 500 stocks. We'll also make things a little easier to solve by narrowing our range of time from 2007-06-30 to 2017-09-30.\n",
    "### Set API Key\n",
    "Set the `quandl.ApiConfig.api_key ` variable to your Quandl api key. You can find your Quandl api key [here](https://www.quandl.com/account/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "\n",
    "# TODO: Add your Quandl API Key\n",
    "quandl.ApiConfig.api_key  = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "snp500_file_path = 'data/tickers_SnP500.txt'\n",
    "wiki_file_path = 'data/WIKI_PRICES.csv'\n",
    "start_date, end_date = '2013-07-01', '2017-06-30'\n",
    "use_columns = ['date', 'ticker', 'adj_close', 'adj_volume', 'ex-dividend']\n",
    "\n",
    "if not os.path.exists(wiki_file_path):\n",
    "    with open(snp500_file_path) as f:\n",
    "        tickers = f.read().split()\n",
    "    \n",
    "    print('Downloading data...')\n",
    "    helper.download_quandl_dataset('WIKI', 'PRICES', wiki_file_path, use_columns, tickers, start_date, end_date)\n",
    "    print('Data downloaded')\n",
    "else:\n",
    "    print('Data already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(wiki_file_path, index_col=['ticker', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Universe\n",
    "We'll be selecting large cap stocks for our stock universe. These stocks are the most liquid and allow to use more buying power with less slippage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Brok): Use large cap stock data\n",
    "percent_top_dollar = 0.2\n",
    "high_volume_symbols = helper.large_dollar_volume_stocks(df, 'adj_close', 'adj_volume', percent_top_dollar)\n",
    "df = df.loc[high_volume_symbols, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-D Matrices\n",
    "In the previous projects, we used a [multiindex](https://pandas.pydata.org/pandas-docs/stable/advanced.html) to store all the data in a single dataframe. As you work with larger datasets, it come infeasable to store all the data in memory. Starting with this project, we'll be storing all our data as 2-D matrices to match what you'll be expecting in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = df.reset_index().pivot(index='ticker', columns='date', values='adj_close')\n",
    "volume = df.reset_index().pivot(index='ticker', columns='date', values='adj_volume')\n",
    "ex_dividend = df.reset_index().pivot(index='ticker', columns='date', values='ex-dividend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "To see what one of these 2-d matrices looks like, let's take a look at the closing prices matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.print_dataframe(close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Smart Beta Portfolio\n",
    "In Part 1 of this project, you'll build a smart beta portfolio using dividend yied. To see how well it performs, you'll compare this portfolio to an index.\n",
    "## Index Weights\n",
    "After building the smart beta portfolio, should compare it to a similar strategy or index. For this project, we'll use a simple index with one of the most common factor of market capitalization.\n",
    "\n",
    "Implement `generate_dollar_volume_weights` to generate the weights for this index. For each date, generate the weights based on market cap for that date. For example, assume the following is market cap data:\n",
    "\n",
    "|          | 10/02/2010 | 10/03/2010 |\n",
    "|----------|------------|------------|\n",
    "| **AAPL** |      2     |      2     |\n",
    "| **BBC**  |      5     |      6     |\n",
    "| **GGL**  |      1     |      2     |\n",
    "| **ZGB**  |      6     |      5     |\n",
    "\n",
    "The weights should be the following:\n",
    "\n",
    "|          | 10/02/2010 | 10/03/2010 |\n",
    "|----------|------------|------------|\n",
    "| **AAPL** |    0.142   |    0.133   |\n",
    "| **BBC**  |    0.357   |    0.400   |\n",
    "| **GGL**  |    0.071   |    0.133   |\n",
    "| **ZGB**  |    0.428   |    0.333   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Brok): Generate weights from market cap\n",
    "# TODO(Brok): Change function name\n",
    "\n",
    "def generate_dollar_volume_weights(close, volume):\n",
    "    \"\"\"\n",
    "    Generate dollar volume weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    volume : str\n",
    "        Volume for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dollar_volume_weights : DataFrame\n",
    "        The dollar volume weights for each ticker and date\n",
    "    \"\"\"\n",
    "    assert close.index.equals(volume.index)\n",
    "    assert close.columns.equals(volume.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    dollar_volume = close * volume\n",
    "\n",
    "    return dollar_volume / dollar_volume.sum()\n",
    "\n",
    "project_tests.test_generate_dollar_volume_weights(generate_dollar_volume_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the index weights using `generate_dollar_volume_weights` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weights = generate_dollar_volume_weights(close, volume)\n",
    "helper.plot_weights(index_weights, 'Index Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETF Weights\n",
    "Now that we have the index weights, it's time to build the weights for the smart beta ETF. Let's build an ETF portfolio that is based on dividends. This is a common factor used to build portfolios. Unlike most portfolios, we'll be using a single factor for simplicity.\n",
    "\n",
    "Implement `calculate_dividend_weights` to returns the weights for each stock based on it's total dividend yield over time. This is similar to generating the weight for the index, but it's dividend data instead of market cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dividend_weights(ex_dividend):\n",
    "    \"\"\"\n",
    "    Calculate dividend weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ex_dividend : DataFrame\n",
    "        Ex-dividend for each stock and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dividend_weights : DataFrame\n",
    "        Weights for each stock and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    dividend_cumsum_per_ticker = ex_dividend.T.cumsum().T\n",
    "\n",
    "    return dividend_cumsum_per_ticker/dividend_cumsum_per_ticker.sum()\n",
    "\n",
    "project_tests.test_calculate_dividend_weights(calculate_dividend_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the ETF weights using `calculate_dividend_weights` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_weights = calculate_dividend_weights(ex_dividend)\n",
    "helper.plot_weights(etf_weights, 'ETF Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns\n",
    "Implement `generate_returns` to generate the returns. Note this isn't log returns. Since we're not dealing with volatility, we don't have to use log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_returns(close):\n",
    "    \"\"\"\n",
    "    Generate returns for ticker and date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns : Dataframe\n",
    "        The returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "\n",
    "    return (close.T / close.T.shift(1) -1).T\n",
    "\n",
    "project_tests.test_generate_returns(generate_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the closing returns using `generate_returns` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = generate_returns(close)\n",
    "helper.plot_returns(returns, 'Close Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Returns\n",
    "With the returns of each stock computed, we can use it to compute the returns for for an index or ETF. Implement `generate_weighted_returns` to create weighted returns using returns and weights for an Index or ETF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_returns(returns, weights):\n",
    "    \"\"\"\n",
    "    Generate weighted returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weights : DataFrame\n",
    "        Weights for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weighted_returns : DataFrame\n",
    "        Weighted returns for each ticker and date\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weights.index)\n",
    "    assert returns.columns.equals(weights.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "\n",
    "    return returns * weights\n",
    "\n",
    "project_tests.test_generate_weighted_returns(generate_weighted_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the etf and index returns using `generate_weighted_returns` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weighted_returns = generate_weighted_returns(returns, index_weights)\n",
    "etf_weighted_returns = generate_weighted_returns(returns, etf_weights)\n",
    "helper.plot_returns(index_weighted_returns, 'Index Returns')\n",
    "helper.plot_returns(etf_weighted_returns, 'ETF Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Returns\n",
    "Implement `calculate_cumulative_returns` to calculate the cumulative returns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate cumulative returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cumulative_returns : Pandas Series\n",
    "        Cumulative returns for each date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return (pd.Series([0]).append(returns.sum()) + 1).cumprod().iloc[1:]\n",
    "\n",
    "project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the etf and index cumulative returns using `calculate_cumulative_returns` and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns)\n",
    "etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns)\n",
    "helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Error\n",
    "In order to check the performance of the smart beta protfolio, we can compare it against the index. Implement `tracking_error` to return the tracking error between the etf and index over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_error(index_weighted_cumulative_returns, etf_weighted_cumulative_returns):\n",
    "    \"\"\"\n",
    "    Calculate the tracking error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_weighted_cumulative_returns : Pandas Series\n",
    "        The weighted index Cumulative returns for each date\n",
    "    etf_weighted_cumulative_returns : Pandas Series\n",
    "        The weighted etf Cumulative returns for each date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tracking_error  : Pandas Series\n",
    "        The tracking error for each date\n",
    "    \"\"\"\n",
    "    assert index_weighted_cumulative_returns.index.equals(etf_weighted_cumulative_returns.index)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    tracking_error = index_weighted_cumulative_returns - etf_weighted_cumulative_returns\n",
    "\n",
    "    return tracking_error\n",
    "\n",
    "project_tests.test_tracking_error(tracking_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the tracking error using `tracking_error` and graph it over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_beta_tracking_error = tracking_error(index_weighted_cumulative_returns, etf_weighted_cumulative_returns)\n",
    "helper.plot_tracking_error(smart_beta_tracking_error, 'Smart Beta Tracking Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Portfolio Optimization\n",
    "In Part 2, you'll optimize the index you created in part 1. You'll use `cvxpy` to optimize the convex problem of finding the optimal weights for the portfolio. Just like before, we'll compare these results to the index.\n",
    "## Covariance\n",
    "Implement `get_covariance` to calculate the covariance of `returns` and `weighted_index_returns`. We'll use this to feed into our convex optimization function. By using covariance, we can prevent the optimizer from going all in on a few stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance(returns, weighted_index_returns):\n",
    "    \"\"\"\n",
    "    Calculate covariance matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weighted_index_returns : DataFrame\n",
    "        Weighted index returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xtx, xty  : (2 dimensional Ndarray, 1 dimensional Ndarray)\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weighted_index_returns.index)\n",
    "    assert returns.columns.equals(weighted_index_returns.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    returns = returns.fillna(0)\n",
    "    weighted_index_returns = weighted_index_returns.sum().fillna(0)\n",
    "\n",
    "    xtx = returns.dot(returns.T)\n",
    "    xty = returns.dot(np.matrix(weighted_index_returns).T)[0]\n",
    "\n",
    "    return xtx.values, xty.values\n",
    "\n",
    "project_tests.test_get_covariance(get_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtx, xty = get_covariance(returns, index_weighted_returns)\n",
    "xtx = pd.DataFrame(xtx, returns.index, returns.index)\n",
    "xty = pd.Series(xty, returns.index)\n",
    "helper.plot_covariance(xty, xtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Programming\n",
    "Now that you have the covariance, we can use this to optomize the weights. Implement `solve_qp` to return the optimal `x` in the convex function with the following constrains:\n",
    "- Sum of all x is 1\n",
    "- x >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Brok): Use cvxpy\n",
    "\n",
    "import cvxopt\n",
    "\n",
    "def solve_qp(P, q):\n",
    "    \"\"\"\n",
    "    Find the solution for minimize 0.5P*x*x - q*x with the following constraints:\n",
    "     - sum of all x equals to 1\n",
    "     - All x are greater than or equal to 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : 2 dimensional Ndarray\n",
    "    q : 1 dimensional Ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : 1 dimensional Ndarray\n",
    "        The solution for x\n",
    "    \"\"\"\n",
    "    assert len(P.shape) == 2\n",
    "    assert len(q.shape) == 1\n",
    "    assert P.shape[0] == P.shape[1]  == q.shape[0]\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    nn = len(q)\n",
    "\n",
    "    g = cvxopt.spmatrix(-1, range(nn), range(nn))\n",
    "    a = cvxopt.matrix(np.ones(nn), (1,nn))\n",
    "    b = cvxopt.matrix(1.0)\n",
    "    h = cvxopt.matrix(np.zeros(nn))\n",
    "\n",
    "    P = cvxopt.matrix(P)\n",
    "    q = -cvxopt.matrix(q)\n",
    "    \n",
    "    # Min cov\n",
    "    # Max return\n",
    "    cvxopt.solvers.options['show_progress'] = False\n",
    "    sol = cvxopt.solvers.qp(P, q, g, h, a, b)\n",
    "\n",
    "    if 'optimal' not in sol['status']:\n",
    "        return np.array([])\n",
    "\n",
    "    return np.array(sol['x']).flatten()\n",
    "\n",
    "project_tests.test_solve_qp(solve_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_optim_etf_weights = solve_qp(xtx.values, xty.values)\n",
    "raw_optim_etf_weights_per_date = np.tile(raw_optim_etf_weights, (len(returns.columns), 1))\n",
    "optim_etf_weights = pd.DataFrame(raw_optim_etf_weights_per_date.T, returns.index, returns.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Portfolio\n",
    "With our optimized etf weights built using quadradic programming, let's compare it to the index. Run the next cell to calculate the optimized etf returns and compare the returns to the index returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_etf_returns = generate_weighted_returns(returns, optim_etf_weights)\n",
    "optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns)\n",
    "helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index')\n",
    "\n",
    "optim_etf_tracking_error = tracking_error(index_weighted_cumulative_returns, optim_etf_cumulative_returns)\n",
    "helper.plot_tracking_error(optim_etf_tracking_error, 'Optimized ETF Tracking Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance Portfolio\n",
    "The optimized etf porfolio used different weights for each day. After calculating in transation fees, this amount of turnover to the portfolio can reduce the total returns. Let's find the optimal times to rebalancve the portfolio instead of doing it every day.\n",
    "\n",
    "Implement `rebalance_portfolio` to rebalance a portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_portfolio(returns, weighted_index_returns, shift_size, chunk_size):\n",
    "    \"\"\"\n",
    "    Get weights for each rebalancing of the portfolio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weighted_index_returns : DataFrame\n",
    "        Weighted index returns for each ticker and date\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    chunk_size : int\n",
    "        The number of days to look in the past for rebalancing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_rebalance_weights  : list of Ndarrays\n",
    "        The etf weights for each point they are rebalanced\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weighted_index_returns.index)\n",
    "    assert returns.columns.equals(weighted_index_returns.columns)\n",
    "    assert shift_size > 0\n",
    "    assert chunk_size >= 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    date_len = returns.shape[1]\n",
    "    all_rebalance_weights = []\n",
    "\n",
    "    for shift in range(chunk_size, date_len, shift_size):\n",
    "        start_idx = shift - chunk_size\n",
    "        xtx, xty = get_covariance(returns.iloc[:, start_idx:shift], weighted_index_returns.iloc[:, start_idx:shift])\n",
    "\n",
    "        all_rebalance_weights.append(solve_qp(xtx, xty))\n",
    "\n",
    "    return all_rebalance_weights\n",
    "\n",
    "project_tests.test_rebalance_portfolio(rebalance_portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "shift_size = 5\n",
    "all_rebalance_weights = rebalance_portfolio(returns, index_weighted_returns, shift_size, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Rebalance Cost\n",
    "With the portfolio rebalanced, we need to use a metric to measure the cost of rebalancing the portfolio. Imeplement `get_rebalance_cost` to calculate the rebalance cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rebalance_cost(all_rebalance_weights, shift_size, rebalance_count):\n",
    "    \"\"\"\n",
    "    Get the cost of all the rebalancing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_rebalance_weights : list of Ndarrays\n",
    "        ETF Returns for each ticker and date\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    rebalance_count : int\n",
    "        Number of times the portfolio was rebalanced\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rebalancing_cost  : float\n",
    "        The cost of all the rebalancing\n",
    "    \"\"\"\n",
    "    assert shift_size > 0\n",
    "    assert rebalance_count > 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    all_rebalance_weights_df = pd.DataFrame(np.array(all_rebalance_weights))\n",
    "    rebalance_total = (all_rebalance_weights_df - all_rebalance_weights_df.shift(-1)).abs().sum().sum()\n",
    "\n",
    "    return (shift_size / rebalance_count) * rebalance_total\n",
    "\n",
    "project_tests.test_get_rebalance_cost(get_rebalance_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_costs = get_rebalance_cost(all_rebalance_weights, shift_size, returns.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potfolio Optimization Results\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Brok): Add plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CODE\n",
    "# THIS CODE IS TEST CODE FOR BUILDING PROJECT\n",
    "# THIS WILL BE REMOVED BEFORE FINAL PROJECT\n",
    "\n",
    "# Error checking while refactoring\n",
    "assert np.isclose(optim_etf_weights, np.load('check_data/po_weights.npy'), equal_nan=True).all()\n",
    "assert np.isclose(optim_etf_tracking_error, np.load('check_data/po_tracking_error.npy'), equal_nan=True).all()\n",
    "assert np.isclose(smart_beta_tracking_error, np.load('check_data/sb_tracking_error.npy'), equal_nan=True).all()\n",
    "\n",
    "# Error checking while refactoring\n",
    "assert np.isclose(unconstrained_costs, 0.10739965758876144), unconstrained_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
