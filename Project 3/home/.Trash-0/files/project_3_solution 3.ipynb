{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Smart Beta Portfolio and Portfolio Optimization\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to use the [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/) packages. Don't import any other packages, otherwise the grader willn't be able to run your code.\n",
    "\n",
    "The other packages that we're importing is `helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.12.0 (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/db/04b13cd69a66657e27dd8af68650b5c8c511501f108358653fca8e52bf86/numpy-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (16.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.8MB 35kB/s  eta 0:00:01  7% |██▌                             | 1.3MB 9.9MB/s eta 0:00:02    15% |█████                           | 2.6MB 14.7MB/s eta 0:00:01    20% |██████▋                         | 3.5MB 16.4MB/s eta 0:00:01    27% |████████▋                       | 4.6MB 24.1MB/s eta 0:00:01    42% |█████████████▊                  | 7.2MB 28.3MB/s eta 0:00:01    49% |████████████████                | 8.4MB 23.3MB/s eta 0:00:01    77% |████████████████████████▋       | 13.0MB 23.6MB/s eta 0:00:01    84% |███████████████████████████     | 14.2MB 26.1MB/s eta 0:00:01    98% |███████████████████████████████▌| 16.5MB 19.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==0.18.1 (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/c0/f0bf4eaef1b6aa7bdd1ae5597ce1d9e729417b3ca085c47d0f1c640d34f8/scipy-0.18.1-cp36-cp36m-manylinux1_x86_64.whl (42.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 42.5MB 12kB/s  eta 0:00:01  5% |█▉                              | 2.4MB 24.0MB/s eta 0:00:02    11% |███▌                            | 4.7MB 24.5MB/s eta 0:00:02    31% |██████████                      | 13.4MB 27.9MB/s eta 0:00:02    33% |██████████▉                     | 14.4MB 17.7MB/s eta 0:00:02    40% |█████████████                   | 17.4MB 19.0MB/s eta 0:00:02    49% |███████████████▉                | 21.1MB 27.3MB/s eta 0:00:01    52% |████████████████▊               | 22.2MB 21.4MB/s eta 0:00:01    83% |██████████████████████████▋     | 35.4MB 16.2MB/s eta 0:00:01    90% |████████████████████████████▉   | 38.4MB 19.7MB/s eta 0:00:01    92% |█████████████████████████████▋  | 39.4MB 22.2MB/s eta 0:00:01    97% |███████████████████████████████▏| 41.3MB 21.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.19.2 (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/33/b455d0af521b76b1982eac1ed1c30c9e67f9885f54c3349aef0b0c547d85/pandas-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (18.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 18.9MB 30kB/s  eta 0:00:01   10% |███▎                            | 2.0MB 23.4MB/s eta 0:00:01    15% |█████                           | 2.9MB 20.2MB/s eta 0:00:01    20% |██████▌                         | 3.8MB 16.5MB/s eta 0:00:01    31% |██████████                      | 5.9MB 18.1MB/s eta 0:00:01    35% |███████████▎                    | 6.7MB 14.4MB/s eta 0:00:01    45% |██████████████▋                 | 8.6MB 22.5MB/s eta 0:00:01    50% |████████████████▎               | 9.6MB 20.6MB/s eta 0:00:01    56% |██████████████████▏             | 10.8MB 23.8MB/s eta 0:00:01    61% |███████████████████▊            | 11.6MB 21.2MB/s eta 0:00:01    77% |█████████████████████████       | 14.7MB 19.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas==0.19.2->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/lib/python3.6/site-packages (from pandas==0.19.2->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2->pandas==0.19.2->-r requirements.txt (line 3))\n",
      "Installing collected packages: numpy, scipy, pandas\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "  Found existing installation: scipy 0.19.1\n",
      "    Uninstalling scipy-0.19.1:\n",
      "      Successfully uninstalled scipy-0.19.1\n",
      "  Found existing installation: pandas 0.20.3\n",
      "    Uninstalling pandas-0.20.3:\n",
      "      Successfully uninstalled pandas-0.20.3\n",
      "Successfully installed numpy-1.12.0 pandas-0.19.2 scipy-0.18.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quandl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d72d643d880c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproject_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/workspace/helper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quandl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper\n",
    "import project_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Data\n",
    "The data source we'll be using is the [Wiki End of Day data](https://www.quandl.com/databases/WIKIP) hosted at [Quandl](https://www.quandl.com). This contains data for many stocks, but we'll just be looking at the S&P 500 stocks. We'll also make things a little easier to solve by narrowing our range of time from 2007-06-30 to 2017-09-30.\n",
    "### Set API Key\n",
    "Set the `quandl.ApiConfig.api_key ` variable to your Quandl api key. You can find your Quandl api key [here](https://www.quandl.com/account/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "\n",
    "# TODO: Add your Quandl API Key\n",
    "quandl.ApiConfig.api_key  = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "snp500_file_path = 'data/tickers_SnP500.txt'\n",
    "wiki_file_path = 'data/WIKI_PRICES.csv'\n",
    "start_date, end_date = '2013-07-01', '2017-06-30'\n",
    "use_columns = ['date', 'ticker', 'adj_close', 'adj_volume', 'ex-dividend']\n",
    "\n",
    "if not os.path.exists(wiki_file_path):\n",
    "    with open(snp500_file_path) as f:\n",
    "        tickers = f.read().split()\n",
    "    \n",
    "    print('Downloading data...')\n",
    "    helper.download_quandl_dataset('WIKI', 'PRICES', wiki_file_path, use_columns, tickers, start_date, end_date)\n",
    "    print('Data downloaded')\n",
    "else:\n",
    "    print('Data already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(wiki_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Universe\n",
    "We'll be selecting dollar volume stocks for our stock universe. This universe is similar to large market cap stocks, because they are the highly liquid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_top_dollar = 0.2\n",
    "high_volume_symbols = helper.large_dollar_volume_stocks(df, 'adj_close', 'adj_volume', percent_top_dollar)\n",
    "df = df[df['ticker'].isin(high_volume_symbols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-D Matrices\n",
    "In the previous projects, we used a [multiindex](https://pandas.pydata.org/pandas-docs/stable/advanced.html) to store all the data in a single dataframe. As you work with larger datasets, it come infeasable to store all the data in memory. Starting with this project, we'll be storing all our data as 2-D matrices to match what you'll be expecting in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = df.reset_index().pivot(index='ticker', columns='date', values='adj_close')\n",
    "volume = df.reset_index().pivot(index='ticker', columns='date', values='adj_volume')\n",
    "ex_dividend = df.reset_index().pivot(index='ticker', columns='date', values='ex-dividend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "To see what one of these 2-d matrices looks like, let's take a look at the closing prices matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.print_dataframe(close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Smart Beta Portfolio\n",
    "In Part 1 of this project, you'll build a smart beta portfolio using dividend yield. To see how well it performs, you'll compare this portfolio to an index.\n",
    "## Index Weights\n",
    "After building the smart beta portfolio, should compare it to a similar strategy or index.\n",
    "\n",
    "Implement `generate_dollar_volume_weights` to generate the weights for this index. For each date, generate the weights based on dollar volume traded for that date. For example, assume the following is dollar volume traded data:\n",
    "\n",
    "|          | 10/02/2010 | 10/03/2010 |\n",
    "|----------|------------|------------|\n",
    "| **AAPL** |      2     |      2     |\n",
    "| **BBC**  |      5     |      6     |\n",
    "| **GGL**  |      1     |      2     |\n",
    "| **ZGB**  |      6     |      5     |\n",
    "\n",
    "The weights should be the following:\n",
    "\n",
    "|          | 10/02/2010 | 10/03/2010 |\n",
    "|----------|------------|------------|\n",
    "| **AAPL** |    0.142   |    0.133   |\n",
    "| **BBC**  |    0.357   |    0.400   |\n",
    "| **GGL**  |    0.071   |    0.133   |\n",
    "| **ZGB**  |    0.428   |    0.333   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dollar_volume_weights(close, volume):\n",
    "    \"\"\"\n",
    "    Generate dollar volume weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    volume : str\n",
    "        Volume for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dollar_volume_weights : DataFrame\n",
    "        The dollar volume weights for each ticker and date\n",
    "    \"\"\"\n",
    "    assert close.index.equals(volume.index)\n",
    "    assert close.columns.equals(volume.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    dollar_volume = close * volume\n",
    "\n",
    "    return dollar_volume / dollar_volume.sum()\n",
    "\n",
    "project_tests.test_generate_dollar_volume_weights(generate_dollar_volume_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the index weights using `generate_dollar_volume_weights` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weights = generate_dollar_volume_weights(close, volume)\n",
    "helper.plot_weights(index_weights, 'Index Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETF Weights\n",
    "Now that we have the index weights, it's time to build the weights for the smart beta ETF. Let's build an ETF portfolio that is based on dividends. This is a common factor used to build portfolios. Unlike most portfolios, we'll be using a single factor for simplicity.\n",
    "\n",
    "Implement `calculate_dividend_weights` to returns the weights for each stock based on its total dividend yield over time. This is similar to generating the weight for the index, but it's dividend data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dividend_weights(ex_dividend):\n",
    "    \"\"\"\n",
    "    Calculate dividend weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ex_dividend : DataFrame\n",
    "        Ex-dividend for each stock and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dividend_weights : DataFrame\n",
    "        Weights for each stock and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    dividend_cumsum_per_ticker = ex_dividend.T.cumsum().T\n",
    "\n",
    "    return dividend_cumsum_per_ticker/dividend_cumsum_per_ticker.sum()\n",
    "\n",
    "project_tests.test_calculate_dividend_weights(calculate_dividend_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the ETF weights using `calculate_dividend_weights` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_weights = calculate_dividend_weights(ex_dividend)\n",
    "helper.plot_weights(etf_weights, 'ETF Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns\n",
    "Implement `generate_returns` to generate the returns. Note this isn't log returns. Since we're not dealing with volatility, we don't have to use log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_returns(close):\n",
    "    \"\"\"\n",
    "    Generate returns for ticker and date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns : Dataframe\n",
    "        The returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "\n",
    "    return (close.T / close.T.shift(1) -1).T\n",
    "\n",
    "project_tests.test_generate_returns(generate_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the closing returns using `generate_returns` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = generate_returns(close)\n",
    "helper.plot_returns(returns, 'Close Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Returns\n",
    "With the returns of each stock computed, we can use it to compute the returns for for an index or ETF. Implement `generate_weighted_returns` to create weighted returns using returns and weights for an Index or ETF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_returns(returns, weights):\n",
    "    \"\"\"\n",
    "    Generate weighted returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weights : DataFrame\n",
    "        Weights for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weighted_returns : DataFrame\n",
    "        Weighted returns for each ticker and date\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weights.index)\n",
    "    assert returns.columns.equals(weights.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "\n",
    "    return returns * weights\n",
    "\n",
    "project_tests.test_generate_weighted_returns(generate_weighted_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the etf and index returns using `generate_weighted_returns` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weighted_returns = generate_weighted_returns(returns, index_weights)\n",
    "etf_weighted_returns = generate_weighted_returns(returns, etf_weights)\n",
    "helper.plot_returns(index_weighted_returns, 'Index Returns')\n",
    "helper.plot_returns(etf_weighted_returns, 'ETF Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Returns\n",
    "Implement `calculate_cumulative_returns` to calculate the cumulative returns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate cumulative returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cumulative_returns : Pandas Series\n",
    "        Cumulative returns for each date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return (pd.Series([0]).append(returns.sum()) + 1).cumprod().iloc[1:]\n",
    "\n",
    "project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the etf and index cumulative returns using `calculate_cumulative_returns` and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns)\n",
    "etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns)\n",
    "helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Error\n",
    "In order to check the performance of the smart beta portfolio, we can compare it against the index. Implement `tracking_error` to return the tracking error between the etf and index over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_error(index_weighted_cumulative_returns, etf_weighted_cumulative_returns):\n",
    "    \"\"\"\n",
    "    Calculate the tracking error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_weighted_cumulative_returns : Pandas Series\n",
    "        The weighted index Cumulative returns for each date\n",
    "    etf_weighted_cumulative_returns : Pandas Series\n",
    "        The weighted etf Cumulative returns for each date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tracking_error  : Pandas Series\n",
    "        The tracking error for each date\n",
    "    \"\"\"\n",
    "    assert index_weighted_cumulative_returns.index.equals(etf_weighted_cumulative_returns.index)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    tracking_error = index_weighted_cumulative_returns - etf_weighted_cumulative_returns\n",
    "\n",
    "    return tracking_error\n",
    "\n",
    "project_tests.test_tracking_error(tracking_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the tracking error using `tracking_error` and graph it over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_beta_tracking_error = tracking_error(index_weighted_cumulative_returns, etf_weighted_cumulative_returns)\n",
    "helper.plot_tracking_error(smart_beta_tracking_error, 'Smart Beta Tracking Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Portfolio Optimization\n",
    "In Part 2, you'll optimize the index you created in part 1. You'll use `cvxopt` to optimize the convex problem of finding the optimal weights for the portfolio. Just like before, we'll compare these results to the index.\n",
    "## Covariance\n",
    "Implement `get_covariance` to calculate the covariance of `returns` and `weighted_index_returns`. We'll use this to feed into our convex optimization function. By using covariance, we can prevent the optimizer from going all in on a few stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance(returns, weighted_index_returns):\n",
    "    \"\"\"\n",
    "    Calculate covariance matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weighted_index_returns : DataFrame\n",
    "        Weighted index returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xtx, xty  : (2 dimensional Ndarray, 1 dimensional Ndarray)\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weighted_index_returns.index)\n",
    "    assert returns.columns.equals(weighted_index_returns.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    returns = returns.fillna(0)\n",
    "    weighted_index_returns = weighted_index_returns.sum().fillna(0)\n",
    "\n",
    "    xtx = returns.dot(returns.T)\n",
    "    xty = returns.dot(np.matrix(weighted_index_returns).T)[0]\n",
    "\n",
    "    return xtx.values, xty.values\n",
    "\n",
    "project_tests.test_get_covariance(get_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's look the the covariance generated from `get_covariance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtx, xty = get_covariance(returns, index_weighted_returns)\n",
    "xtx = pd.DataFrame(xtx, returns.index, returns.index)\n",
    "xty = pd.Series(xty, returns.index)\n",
    "helper.plot_covariance(xty, xtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Programming\n",
    "Now that you have the covariance, we can use this to optimize the weights. Implement `solve_qp` to return the optimal `x` in the convex function with the following constraints:\n",
    "- Sum of all x is 1\n",
    "- x >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "def solve_qp(P, q):\n",
    "    \"\"\"\n",
    "    Find the solution for minimize 0.5P*x*x - q*x with the following constraints:\n",
    "     - sum of all x equals to 1\n",
    "     - All x are greater than or equal to 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : 2 dimensional Ndarray\n",
    "    q : 1 dimensional Ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : 1 dimensional Ndarray\n",
    "        The solution for x\n",
    "    \"\"\"\n",
    "    assert len(P.shape) == 2\n",
    "    assert len(q.shape) == 1\n",
    "    assert P.shape[0] == P.shape[1]  == q.shape[0]\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    nn = len(q)\n",
    "\n",
    "    g = cvxopt.spmatrix(-1, range(nn), range(nn))\n",
    "    a = cvxopt.matrix(np.ones(nn), (1,nn))\n",
    "    b = cvxopt.matrix(1.0)\n",
    "    h = cvxopt.matrix(np.zeros(nn))\n",
    "\n",
    "    P = cvxopt.matrix(P)\n",
    "    q = -cvxopt.matrix(q)\n",
    "    \n",
    "    # Min cov\n",
    "    # Max return\n",
    "    cvxopt.solvers.options['show_progress'] = False\n",
    "    sol = cvxopt.solvers.qp(P, q, g, h, a, b)\n",
    "\n",
    "    if 'optimal' not in sol['status']:\n",
    "        return np.array([])\n",
    "\n",
    "    return np.array(sol['x']).flatten()\n",
    "\n",
    "project_tests.test_solve_qp(solve_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to generate optimal weights using `solve_qp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_optim_etf_weights = solve_qp(xtx.values, xty.values)\n",
    "raw_optim_etf_weights_per_date = np.tile(raw_optim_etf_weights, (len(returns.columns), 1))\n",
    "optim_etf_weights = pd.DataFrame(raw_optim_etf_weights_per_date.T, returns.index, returns.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Portfolio\n",
    "With our optimized etf weights built using quadratic programming, let's compare it to the index. Run the next cell to calculate the optimized etf returns and compare the returns to the index returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_etf_returns = generate_weighted_returns(returns, optim_etf_weights)\n",
    "optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns)\n",
    "helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index')\n",
    "\n",
    "optim_etf_tracking_error = tracking_error(index_weighted_cumulative_returns, optim_etf_cumulative_returns)\n",
    "helper.plot_tracking_error(optim_etf_tracking_error, 'Optimized ETF Tracking Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance Portfolio\n",
    "The optimized etf portfolio used different weights for each day. After calculating in transaction fees, this amount of turnover to the portfolio can reduce the total returns. Let's find the optimal times to rebalance the portfolio instead of doing it every day.\n",
    "\n",
    "Implement `rebalance_portfolio` to rebalance a portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_portfolio(returns, weighted_index_returns, shift_size, chunk_size):\n",
    "    \"\"\"\n",
    "    Get weights for each rebalancing of the portfolio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weighted_index_returns : DataFrame\n",
    "        Weighted index returns for each ticker and date\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    chunk_size : int\n",
    "        The number of days to look in the past for rebalancing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_rebalance_weights  : list of Ndarrays\n",
    "        The etf weights for each point they are rebalanced\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weighted_index_returns.index)\n",
    "    assert returns.columns.equals(weighted_index_returns.columns)\n",
    "    assert shift_size > 0\n",
    "    assert chunk_size >= 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    date_len = returns.shape[1]\n",
    "    all_rebalance_weights = []\n",
    "\n",
    "    for shift in range(chunk_size, date_len, shift_size):\n",
    "        start_idx = shift - chunk_size\n",
    "        xtx, xty = get_covariance(returns.iloc[:, start_idx:shift], weighted_index_returns.iloc[:, start_idx:shift])\n",
    "\n",
    "        all_rebalance_weights.append(solve_qp(xtx, xty))\n",
    "\n",
    "    return all_rebalance_weights\n",
    "\n",
    "project_tests.test_rebalance_portfolio(rebalance_portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to rebalance the portfolio using `rebalance_portfolio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "shift_size = 5\n",
    "all_rebalance_weights = rebalance_portfolio(returns, index_weighted_returns, shift_size, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Rebalance Cost\n",
    "With the portfolio rebalanced, we need to use a metric to measure the cost of rebalancing the portfolio. Implement `get_rebalance_cost` to calculate the rebalance cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rebalance_cost(all_rebalance_weights, shift_size, rebalance_count):\n",
    "    \"\"\"\n",
    "    Get the cost of all the rebalancing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_rebalance_weights : list of Ndarrays\n",
    "        ETF Returns for each ticker and date\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    rebalance_count : int\n",
    "        Number of times the portfolio was rebalanced\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rebalancing_cost  : float\n",
    "        The cost of all the rebalancing\n",
    "    \"\"\"\n",
    "    assert shift_size > 0\n",
    "    assert rebalance_count > 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    all_rebalance_weights_df = pd.DataFrame(np.array(all_rebalance_weights))\n",
    "    rebalance_total = (all_rebalance_weights_df - all_rebalance_weights_df.shift(-1)).abs().sum().sum()\n",
    "\n",
    "    return (shift_size / rebalance_count) * rebalance_total\n",
    "\n",
    "project_tests.test_get_rebalance_cost(get_rebalance_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to get the rebalance cost from  `get_rebalance_cost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_costs = get_rebalance_cost(all_rebalance_weights, shift_size, returns.shape[1])\n",
    "print(unconstrained_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CODE\n",
    "# THIS CODE IS TEST CODE FOR BUILDING PROJECT\n",
    "# THIS WILL BE REMOVED BEFORE FINAL PROJECT\n",
    "\n",
    "# Error checking while refactoring\n",
    "assert np.isclose(optim_etf_weights, np.load('check_data/po_weights.npy'), equal_nan=True).all()\n",
    "assert np.isclose(optim_etf_tracking_error, np.load('check_data/po_tracking_error.npy'), equal_nan=True).all()\n",
    "assert np.isclose(smart_beta_tracking_error, np.load('check_data/sb_tracking_error.npy'), equal_nan=True).all()\n",
    "\n",
    "# Error checking while refactoring\n",
    "assert np.isclose(unconstrained_costs, 0.10739965758876144), unconstrained_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
